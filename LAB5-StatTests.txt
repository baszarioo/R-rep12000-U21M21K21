//// STATISTICAL TESTS IN R + t-STUDENT TESTS \\\\============

T-STUDENT TESTS:
They are parametric tests (related to distribution parameters) for one or two samples, involving the testing hypotheses concerning an unknown expected value (also known as significance tests).
Assumptions for application: measurements follow a normal distribution or the sample size is large (sometimes n>30 is sufficient(wystarczajacy)).

The t.test() function allows for the execution of the t-Student test. The first argument of the function is the data vector. The following important arguments (optional):
* y = the second vector with data,
* alternative = two.sided / less / greater = specifies the alternative hypothesis (default: two-sided)
* mu = the number specifying the expected value of the distribution being tested (or the difference in expected values in the case of two samples), default is 0.
* paired = TRUE, FALSE = specifies whether the test should be conducted(przeprowadzony) for: dependent(TRUE) or independent (FALSE = default) data. If paired=TRUE, the data vectors must be of equal length.

NOTE: In cases where the assumptions of the test sare not met, an alternative to the t-Student test is the Wilcoxon test.



GENERAL INFERENCE SCHEME: (ogl. schemat. wnioskowania)
1. Formulate two mutually exclusive (wzajemnie wykluczające się) hypotheses: H0 (null) and H1(alternative); together, they civer all possible cases according to us. We will consider only the situation where  the H0 hypothesis is simple, and the H1 hypothesis is complex. A simple hypothesis is called a parametric hypothesis for which one parameter value corresponds (odpowiada jej tylko jedna wart. parametru), while a complex hypothesis is called a hypothesis for which more than one parameter value corresponds.

2. Determine the significance level(istotności poz.) of the test (alfa) [>0 && <1] (usually (alfa-0.05)); We always do this at the beginning of the inference (wnioskowanie). This is the probability of making a so-called Type 1 error. => H0 is true, but we reject it; Type II error - H1 is true, but we decide in favor of H0.

    REALITY/DECISION        ACCPET H0       ACCEPT H1
     H0 true,                   OK         Type 1 error
     H1 true,              Type II error,       OK



GERNERAL INFERENCE SCHEME, advance:
In the table, all situation that may occur have been descirbed. In two cases, we do not make erros: H0 is true, and our decision is in favor of H1. In the remaining two cases, our decisions are incorrect.
We would like the probabilities of making both errors to be as small as possible. It turns out that this cannot be done simultaneously (jednocześnie)  ; reducing the probability of making one error increases the probability of making the other error, and vice versa. ThereforeI(wobec tego) we establish the following rule of inference (J. Neyman, E. Pearson): " We recognize the that committing a Type 1 error has worse consequences than a Type II error, so we primarily control the probability of making a Type I error, by setting the maximum acceptable value of this probability (i.e, alfa) at the very beginning. What abot the Type II error?...



GENERAL INFERENCE SCHEME, advance part2:
For Type II errors, we aim to make it smaller as well, but we don't worry much about it.

3. We choose a statistic,  which is a funciton of the sample or samples (this statistic is called teh test statistic) whose distribution, in the case of the truth of the H0 hypothesis, we can determine (it cannot depend on unknown parameters).
The decision about which hypothesis, according to use, is true (ma miejsce), is made based on the value of the statistic.

There are 2 equivalent ways to make a decision.
 * Using a critical region (manual);
 * Using the p-value (computerized).
Let's consider both methods, starting with the first one.



STUDENT'S T-TEST: SINGLE SAMPLE:
Example1: According to technical standards, the machining of a single steel ring should take the grinder 22 minutes. Sixteen wrokstations were renadomly selected, and the machining times were as follows: 23, 22, 24, 23, 26, 25, 24, 23, 23, 27, 23, 22, 24, 25 and 24 minutes.
+ Assuming(zakładając) that the machining time(czas obrobki) follows a normal distribution, verify the hypothesis at a significance level of alfa=0.05:, Hypothesis: H0 = \mu - 22, against the alternative hypotheses:
a) H_1 : \mu != 22
b) H_1 : \mu < 22
c) H_1 : \mu >22

Let's denote the tested value as \mu_9, i.e, \mu_0 = 22. In this test, the test statistic takes the form:
[ T_n = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}} ].



EXAMPLE 1:
where ( n = 16 ) is the sample size, ( \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i ) is the sample mean, and ( s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2} ) is the sample standard deviation.
We have:
[ \bar{x} = \frac{1}{16} (23 + \ldots + 24) = 24 ]
[ s = \sqrt{\frac{1}{15} ((23 - 24)^2 + \ldots + (24 - 24)^2)} \approx 1.46 ]
Therefore, the test statistic is:
[ T_n = \frac{\sqrt{n}(\bar{x} - \mu_0)}{s} = \frac{\sqrt{16}(24 - 22)}{1.46} \approx 5.48 ]


The critical set is a subset of the set of real numbers, and the probability of the test statistic falling into this set, given the truth of the null hypothesis (H_0), is (\alpha); in out case, (\alpha=0.05). We can define such sets in many ways. In Student's t-tests, we use three types of critical sets: two-sided, left-sided, and right-sided. The type of critical set we use determines the form of the alternative hypothesis.



EXAMPLE1: CRITICAL SET adv. 8;/20
a) Alternative hypothesis (H_1: \mu != 22.)
From the quantile table of the t-Student distribution we obtain: (F^(-1){t(n-1)} (1- \frac(\alpha){2}) = F^{-1} {t(15)}(0.975) = 2.131) since (n-1=15). The notation (F^{-1}{t{n-1}}(1 - \frac{\alpha}{2})) comes from the fact that this number is the root of the equation (F_(t_(n-1))(t) = 1- \frac(\alpha){2}), where (F_(t_(n-1))) denotes the cumulative distribution function of the t-Student distribution. The critical set (two-sided) is given by:
 K = \left(-\infty, -F^{-1}{t{n-1}}(1 - \frac{\alpha}{2})\right) \cup \left(F^{-1}{t{n-1}}(1 - \frac{\alpha}{2}), +\infty\right) = (-\infty, -2.131) \cup (2.131, +\infty). ]

Decision: (T_n \in K), therefore, we reject (H_0) in favor of (H_1).



EXAMPLE1: CRITICAL SET adv. part 2:
(b) Alternative hypothesis \(H_1: \mu < 22\).

From the quantile table of the t-Student distribution, we obtain:
\[ F^{-1}_{t_{n-1}}(1 - \alpha) = F^{-1}_{t_{15}}(0.95) = 1.753. \]
The critical set (left-sided) is given by:
\[ K = \left(-\infty, -F^{-1}_{t_{n-1}}(1 - \alpha)\right) = (-\infty, -1.753). \]
Decision: \(T_n \notin K\), therefore, we do not have grounds to reject \(H_0\).

(c) Alternative hypothesis \(H_1: \mu > 22\).

From the quantile table of the t-Student distribution, we obtain:
\[ F^{-1}_{t_{n-1}}(1 - \alpha) = F^{-1}_{t_{15}}(0.95) = 1.753. \]
The critical set (right-sided) is given by:
\[ K = \left(F^{-1}_{t_{n-1}}(1 - \alpha), +\infty\right) = (1.753, +\infty). \]
Decision: \(T_n \in K\), therefore, we reject \(H_0\) in favor of \(H_1\).

Note: If the significance level of the test (\(\alpha\)) is not specified in the task, we take the standard level, i.e., \(\alpha = 0.05\).



P-VALUE:
But how do we make decision in the R envirionment? We use the p-values.
Statistical tests conducted in the R environment do not immediately and unequivocally provide an answer on whether to accept or reject the null hypothesis. Instead, we obtain a p-value.
What is a p-value? If the observed value of the test statistic (T_n) is denoted as ( t_{\text{obs}}), the p -value is determined as follows;
[ P(T_n > t_{\text{obs}}) \text{, in the case of an alternative hypothesis with the > sign;} ]
[ P(T_n < t_{\text{obs}}) \text{, in the case of an alternative hypothesis with the < sign;} ]
[ P(T_n > |t_{\text{obs}}|) + P(T_n < -|t_{\text{obs}}|) = 2P(T_n > |t_{\text{obs}}|) = 2P(T_n < -|t_{\text{obs}}|)
\text:{, in the case of an alternative hypothesis with the } \neq \text {sign.} ]

The p-valie allows us to assess credibility[określic wiarygodność] of the null hypothesis H_0:
the larger the p-value, the more we should accept the null hypothesis. A small p-value goes agains the null hypothesis.
To be more precise, the decision to accept or reject (H_0) is made by comparing the p-value with the significance level (\alpha) of the test:
[ \text{If } p \leq \alpha, \text{ reject } H_0; ]
[ \text{If } p > \alpha, \text{ accept } H_0. ]



P-VALUE; adv.
We reject the null hypothesis when (p < \alpha) <==> (t_{\text{obs|) \in K );\
 we do not have basis(pdostaw) to reject the null hypothesis when:
(p > \alpha) <==> (t_{\text{obs}} \notin    (!e)    K).
    In the case of a two-sided critical region.

Note: both the siginficance level (\alpha) and the p-value are numbers in the interval (0,1).



EXAMPLE 1 IN R:!
> x=c(23,22,24,23,26,25,26,24,23,23,27,23,22,24,25,25);
# To check the result using the ""manual" method, we can calculate the test statistic value from the formula mentioned above:
> n=length(x);
> T=sqrt(n)*(mean(x)-22)/sd(x);
# Of course, we obtain the same result as above.

# Now, let's do it directly in the R environment.
#1. If the alternative hypothesis H1 is \mu !=22(case (a))"
> t.test(x, mu=22);
# Since p-value is = 6.372e-05 < 0.05, we reject H0 in favor of H1.


EXAMPLE 1 IN R: adv.:
2. If the alternative hypothesis H1 is \mu < 22 (case (B)):
> t.test(x, mu=22, alternative = "less");
# Since p-value = 1>0.05, we do not have grounds/basis to reject the null hypothesis.

3. If the alternative hypothesis H1 is: \mu > 22 (case (C)):
> t.test(x, mu=22, alternative = "greater");
#since p-value if 3.186e-05 < 0.05, we reject H0 in favor of H1.



STUDENT'S t-TEST: Single Sample:
Example 2; Let's generate a random sample from a uniform distribution on the interval (-1, 1). First, let's test whether we cam assume that the expected value of the distribution from which the sample comes is -2:
> x = runif(50, -1, 1)
> t.test(x, mu=-2)

Since the p-value is siginificantly smaller than the standard significance level of 0.05, we reject the hypothesis of a mena equal to -2 in favor of the hypothesis of a mean not equal to -2.
Let's see what we get when testing the expected valueq equal to 0 (mu=0 is the default value):
> t.test(x);

As expected, the p-value is greater than 0.05, so we do not have basis to reject the null hypothesis.
Therefore, we can asuume that the expected vlaue of the distribution from which the data comes is -.
Note: The sample does not come from a normal distribution, but (n=50).



STUDENT'S t-TEST: TWO INDEPENDENT SAMPLES.
In the case of the t-Student test for two independent samples, the null hypothesis H0 is: the expected values of the distributions of both samples are the same.
H_1 is: the expected values of the distributions of the samples are different (or the expected value for the first sample is less than the expected for the second sample, or vice versa).

Exmpl:3/ We are given two samples.
> x1 = c(50,48,39,53,51,49,50,55,47,53);
> x2 = c(46,54,58,49,52,51,58,46);
- Assuming that both samples come from normal distributions, we want to check whether the expected values of the distributions of both samples are the same. Since the samples have different sizes, they are certainly indepndent samples.
The standard t-Student test assumes that the variances of the distributions from which the samples come are equal. Therefore, let's check if this is the case.
We apply the test for equality of variances:
> var.text(x1, x2);
