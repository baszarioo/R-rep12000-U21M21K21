//Compatibility_Tests//;
These are tests that check the compatibility of the sample distribution with a certain establishedor theoretical distribution. Popular tests in R include the:
 - Chi-square test, which can be used to test both discrete and absolutely continuous distributions, but is better suited for testingg discrete distributions.
 - The Kolmogorov-Smirnov test is used to test absolutely continuous distributions.
Additionally in R, we have test available to test the compatibility of the sample distribution with a normal distribution, such as the Shapiro-Wilk's test.


== CHI-SQUARE GOODNESS-OF-FIT TEST: ==
This test is conducted using the 'chisq.test()' function; testis based on the chi-square statistic.
The null hypothesis states that the distribution from which the sample is drawn is the specified one, and the alternative hypothesis suggests that the distribution is different. -unlike t-student test, there are no different types of alternative hypotheses here/
To perform this test, you must first prepare the data apopropriately. The observed values from the sample must be grouped into classes (either as a point or interval frequency distribution), so that the observed class frequencies can be compared with the expected frequencies (i.e., frequencies expected under the assumption that the data come from the specified distribution in H0 ).
NOTE: the test result is considered most reliable when the expected frequencies for each class are not less than 10 (expected frequency is the theoretical number of observations in the class according to the distribution assumed in H0).
==part2==
The first argument of the chisq.test() function is usually a frequency table obtained by using the table() function. If no other argument is provided, the function defaults to testing the null hypothesis that the distribution is uniform (expected frequencies  for each class are equal). If we do not want to test equal proportions, they should be specified as an additional argument:
 * p: a vector of probabilities( of the same size as the number of different values in the sample). By default, this is a vector of probabilities for a uniform distribution, meaning each class has an equal probability of 1/k, where k is the number of classes.
 * rescale.p=TRUE if the components of the probability vector p do not sum to 1 and need to be rescaled (default is FALSE).
==example==
Example. Let's conduct a chi-square goodness-of-fit test with a Poisson distribution for data from the file "dane.csv." We start by loading the file into R:
> data = read.table("dane.csv", sep=";")
The Poisson distribution is a family of distributions indexed by a positive parameter λ. Here arises the first issue: the chi-square goodness-of-fit test checks conformity with a specific distribution, not a family. To specify the Poisson distribution, we need to choose the parameter λ. Since λ is unknown, we use its estimator as λ. As the expected value for the Poisson distribution is λ, a good estimator for the parameter λ is the sample mean:
> lambda = mean(data$x) # Estimating λ with the sample mean
Now, let's see how many different values are present in the sample:
> table_result = table(data)
==example continuum==
Here we encounter a second problem: a random variable with a Poisson distribution can take any non-negative integer value, butr in our sample only values 0,1,2,3,4,5 occur. What should we do?
Let's examine the probabilities of taking values 0,1,2,3,4,5 in the case of a Poisson distributino with parameter (lambd)=1.7;
> prob=c() # create a vector of probabilites for Poisson distribution
> for (i in 0:5) { prob[i+1]=dpois(i,1) }
> sum(prob);
--We see that the sum of these six probabilities is 0.9920006, which is almost one. Therefore, it can be approximately assumed that for this distribution, the remaining values (6,7,8,...) are not taken. Hence, we apply the chi-square goodness-of-fit fit test:
> chisq(.test(t, p=prob, rescale.p=TRUE))


== KOLMOGOROV-SMIRNOV TEST ==
We obtain a p-value equal to 0.644. Therefore we conclude that there is no evidence to suggest that the null hypothesis is not true, meaning there is consistency between the data and a Poisson distribution.
Note: The probability vector must be explicitly specified by using (p=).
The " ks.test() " function allows for conducting a one sample or two-sample kolmogorov-smirnov test for distributional equality. The first argument of the function is a vector containing the sample, and the subsequent aruments are:
 - a character string specifying the cumulative distribution function of the tested absolutely continuous distribution = in the case of a one-sample test, or a vector with the second sample if we are testing the
                equality of distributions, between two samples.
 - alternative: two.sided / less / greater = specifies the alternative hypothesis. (default: two-sided)
==one-sample: test example==
* exact= TRUE(default) or FALSE - a logical value specifying whether the exact p-value of the test should be
        calculated (option unavailable for one-sided two-sample tests or when tere are ties, i.e., identical values in the sample).
Note: I nthe case of absolutely continuous distributions, the presence of ties can be concerning (often arising from the precision of rounding numbers) and may significantly influence the test result!
Example: Let's compare the fit of a randomly generated sample from a normal distribution N(1,1) first with a uniform distribution and then with a normal distribution:
> sample = rnorm(50, mean=1);
> ks.test(sample, punif, min(sample), max(sample));
> ks.test(sample, pnorm, mean(sample));
Note: In the first case, we specify the tested uniform(jednostajny) distribution as the distribution over the interval (min(sample), max(sample));
It's considered mentioning that the endpoints of the interval are parameters of the uniform distribution, and the selected values min(sample) and max(sample) serve as estimates for these endpoints. In the second case, we specify the tested normal distribution by selecting the sample mean as an estimate for the expected value (after all, we assume that we do not know the distribution from which the sample comes).
The result of testing the sample distribution as uniform: the p-value is very samll, and we should reject the null hypothesis of conformity ot the uniform distribution, although for some significance levels, the null hypothesis may not be rejected due to the small sample size.
    However, in testing the normality of the distribution, the result is more decisive(zdecydow.): the p-value is large, and we should not reject the null hypothesis conformity to the normal distribution.
==Cumulative Distribution Function (CDF) plots of K-S test==
The Kolmogorov-Smirnov Test is base on a statistic that relies on the distance between the empirical cumulative distribution function (ECDF) of the sample and the theoretical cumulative distribution function.
Let's examine how the corresponding CDF plots look:
> plot.ecdf(sample)
> curve(pnorm(x, mean(sample)), add=TRUE, col="red");
> curve(punif(x, min(sample), max(sample)), add=TRUE, col="blue");
 The conclusion is evident: the difference between the empirical cumulative distribtuiuon function of the sample and the cumulative distribution function of the uniform distribution is large, but when compared to the cumulative distribution function of the normal distribution, this difference is small.

==Two-Sample Kolmogorov-Smirnov Test: Example==
Example: Let's compare the distribution agreementof reandomly generated samples from a uniform distribution U(0, 10) and an exponential standard distribution E(1):
> x = runif(20, min=0, max=10)
> y = rexp(30)
> ks.test(x,y)
A very small p-value indicates that the samples do not come from the same distributions (rejecting the null hypothesis of equality of sample distrtibutions), which could have been expected. Let's look at completely different empirical cumulative distrtibution function plots:
> plot.ecdf(x, col="red");
> plot.ecdf(y, col="blue", add=TRUE);
==2-Sample K-S Test: Example 'One sided alternatives'==
In the previous example, it can be observed that the empirical cumulative distribution function of sample x lies below the empirical cumulative distribution function of sample y. In such a case, we can conduct the K-S test with one-sided alternatives. Let's first test the null hypothesios of equality between the distributions of x and y against the alternative hypothesis stating that the distribution function of x is not less (lies above) than the distribution function of y (x is stochastically less than y):
> ks.test(x,y,alternative="g");
We obtained a p-value significant enough that we cannot reject the null hypothesis of equality between the distributions of x and y.
Now, let's test the null hypothesis of equality between the distributions of x and y against the alternative hypothesis stating that the distribution function of y(x is stochasitacally greater [without any tendence/ ruled by a circumstance] than y).
> ks.test(x,y,alternative="l");
The obtained p-value is small enough to reject the hypothesis of equality of distributions in favor of one stating that the distribution function of x is smallet than the distribution function of y.

==Shapiro-Wilk test==
This test is a fundamental test for normality conformity.
We conduct this test using the shapiro.test() function. Examples:
> x =rgamma(50,3,3);
> sapiro.test(x);
As expected, the p-value is small enough to reject the null hypothesi, idnicating that sample x does not come from a normal distribution.
> y =rnorm(20,3,2);
> shapiro.test(y);
In this case, there is no doubt - the test result does not allow us to reject the null hypothesis (p-value is large). Therefore, we accept that the distribution from which sample y is drawn - is normal.

==Other Tests of Normaliy - nortest package==
In the R environment, many other tests are available to assess normality conformity, some of which are modifications of the Kolmogorov-Smirnov test. These tests can be found in the nortest package:
- Anderson-Darling test: function ad.test(),
- Cramer-Von Mises test: function cvm.test(),
- Lilliefors test: function lillie.test() [this is a K-S test with Lilliefors correction],
- Pearson's chi-square test for normality: function pearson.test(),
- Shapiro-Francia test: function sf.test()

==How to check the Normality of Distribution?==
General recommendations for checking the normality of a sample distribution are as follows:
    1. Conduct basic statistical analysis: calculate skewness (should be close to 0) and kurtosis (should be close to 3).
    2. Create a histogram, boxplot, and quantile-quantile plot.
If the sample siez is not very large (below 5000), use the Shapiro-Wilk test.
If the number of observations is +5000, use the Anderson-Darling test (or the K-S test with Lilliefors correction).

== sttmnts On Normality Testing ==
It's important to note that no test can directly confirm that data comes from a normal distribution! A test is only able to indicate when data deviates significantly from a normal distribution, and in such cases, the null hypothesis of normality should be rejected.
Examples: When the sample size is small, even substantial deviations(duże odchylenia) from normality may go undetected.
> x = rbinom(10,5,0.6);
Here, we generated a sample from a binomial distribution(rozklad dwumianowy), which is discrete distribution and is not close to a normal distribution in any way. Let's conduct a test for the normality of the sample distribution:
> shapiro.test(x);
The result indicates that we have no grounds to reject the null hypothesis of the normalityof the sample distribution!
== c.d Another example ==
>x=rlnorm(20,0,0.4);
we generated a sample from a log-normal distribution, which is not a normal distribution. The normality test:
> shapiro.test(x);
#once again indicates that we have no grounds to reject the null hypothesis of the normality of the sample distribution.
Example:In the case of large samples, even a small deviation from normality can lead to rejecting tthe null hypothesis of normality. Let's install the nortest' package:
>install.packages("nortest");
>library(nortest);
Let's generate a very large sample of size 500k from a Student's t-distribution with 200 degrees of freedom.
> x = rt(500000, 200);
== c.d ==
A studnet's t-distribution with such degrees of freedom is indistinguishable(nieodroznialny) from a normal distribution. To confirm these words, one can refer to the histogram or quantile-quantile plot:
> hist(x);
> qqnorm(x)
#However, let's check the result of the normality test for the sample( using, for example, the Anderson-Darling test, as the sample size is very large):
> ad.test(x);
However, a small p-value suggests rejecting the null hypothesis of normality for the sample distribution...
